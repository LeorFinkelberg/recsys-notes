### Accuracy vs Beyond-accuracy

Accuracy оценивает насколько точно алгоритм предсказывает "ground truth". Ground truth -- это пары user-item, которые действительно встречались в данных, то есть было взаимодействие между user-item.

Beyond-accuracy -- это метрики, которые важны "после точности" (при прочих равных). Если две модели дают одинаковые метрики точности, но вторая смогла рекомендовать менее популярные item/более разнообразные категории -- она, скорее всего лучше. Максимизация beyond-accuracy метрик важна при сохранении точности. 

Между accuracy и beyond-accuracy обычно есть размен (отдаешь точность, повышаешь новизну). Пример можно посмотреть здесь https://arxiv.org/pdf/2403.04875

Основные метрики качества:
_Accuracy_:
- NDCG@K / MAP@K / MRR@K измеряют, насколько близко ground truth объекты находятся близко к началу списка.
- Recall@K, Precision@K / HitRate@K -- метрики оценивают список из $k$ элементов целиком (метрика не поменяется, если внутри $k$ элементов перемешать список).

_Beyond-accuracy_:
- Diversity (разнообразие). Есть как минимум 3 вида: 1) внутри списка одного пользователя (чтобы не показывать item только одного типа), 2) про отличие рекомендаций между пользователями (то есть не показываем ли мы всем одно и то же), 3) рекомендации у одного пользователя с течением времени (меняется ли у пользователя лента или нет).
- Popular / novelty показывает, насколько популярные / новые item в среднем рекомендуются. Если популярность меньше, от этого, как правило, много плюсов.
- User/Item Coverage показывает какая доля пользователей / item используется и показывается.
- Fairness. Насколько items одинаково хорошо рекомендуются. Правда ли, что у пользователей одинаково хорошая персонализация? Например, рандомные рекомендации одинаково честны ко всем items -- они дают одинаковый процент влияния.
- Serendipity. Оценивает способность рекомендательной системы генерировать рекомендации с вау-эффектом. Нет точного понимания как это считать единственно верным способом.

Метрики качества считаются в двух основных вариантах.

Retrieval оценивает качество генерации кандидатов, либо просто "угадывание" items. Множество рекомендуемых items -- максимальное возможное (весь каталог, или то, что может порекомендовать модель). Ground truth -- это реальные взаимодействия пользователя, и их нужно угадать. Но ключевое: модель может порекомендовать все что угодно.

Ranking. У нас есть семплы user-item-timestamp-label. Это могут быть просмотры какой-то ленты, или просто факты того, что пользователь видел. Задача алгоритма -- отранжировать items из этого множества, чтобы вверху оказались полезные действия (клики/лайки/покупки). Отличие от retrieval: в ranking сетапе модель ранжирует только те item, которые пользователь, скорее всего видел, и множество item сильно сужено.

Иногда в статьях делают negative sampling для оценки метрик. Берут позитивный item, и рандомно / по популярности 100 items из каталога. Ранжируют одно против другого. Это может привести к плохим последствиям.