Ранжирование -- это задача сортировки набора элементов их релевантности. Чтобы оценить качество ранжирования нужно иметь некоторый эталон, с которым можно было бы сравнить результаты алгоритма. 

Существует два основных способа получения $r_{true}$:
- на основе исторических данных; например, в случае рекомендации контента, можно взять просмотры (лайки, покупки) пользователя и присвоить просмотренным элементам 1, а остальным 0.
- на основе экспертной оценки; например, в задаче поиска, для каждого запроса можно привлечь команду ассесоров, которые вручную оценят релевантности документов запросу.
### Precision top K (P@K)

Precision top K -- это точность на $K$-элементах. Пусть алгоритм ранжирования выдал оценки релевантности для каждого элемента $r(e)_{e \in E}$ . Отобрав среди них первые $K \leqslant M$ элементов с наибольшим $r(e)$ можно посчитать долю релевантных
$$
	p@K = \dfrac{ \sum_{k=1}^K r^{true}(\pi^{-1}(k)) }{K} = \dfrac{Number\,of\,relevant\,items\,in\,K}{K},
$$
где $r^{true}$ -- индикаторная функция (0 или 1); $\pi^{-1}(k)$ -- элемент, который в результате перестановки оказался на $k$-ой позиции.

==P@K не учитывает порядок элементов в топе==. Так, если из 10 элементов мы угадали один, то неважно на каком месте он находится -- на первом или на последнем, в любом случае $p@k = 0.1$. Хотя очевидно, что первый вариант лучше.

Этот недостаток устраняет метрика _Average Precision_(ap@K)
$$
ap@K = \dfrac{1}{K} \sum_{k=1}^K r^{true}(\pi^{-1}(k)) \, p@k.
$$
Так, если из 3 элементов релевантным оказался только тот, что на последнем месте, то $1/3 \, (0/1 + 0 / 2 + 1 / 3) = 0.(1)$. А если угадали лишь тот, что был на первом месте, то $1/3 (1/1 + 0/2 + 0/3) = 0.(3)$.
### Mean Average Precision

MAP одна из наиболее часто встречающихся метрик качества ранжирования. В p@K и ap@K качество ранжирования оценивается для отдельно взятого объекта (пользователя, поискового запроса). На практике объектов множество: мы имеем дело с сотнями тысяч пользователей, миллионами поисковых запросов и тд. Идея MAP@K заключается в том, чтобы посчитать ap@K для каждого объекта и усреднить
$$
MAP@K = \dfrac{1}{N} \sum_{j=1}^N ap@K_j.
$$
Идея вполне логична, если предположить, что все пользователи одинаково важны. Если же это не так, то вместо простого усреднения можно использовать взвешенное, домножив ap@K каждого объекта на соответствующий вес его важности.
### Cumulative Gain at K

Снова рассмотрим один объект и $K$ элементов с наибольшим $r(e)$. Cumulative Gain at K (CG@K) -- базовая метрика ранжирования, которая использует простую идею: чем релевантнее элементы в топе, тем лучше
$$
CG@K = \sum_{k=1}^K r^{true}(\pi^{-1}(k)).
$$
==Очевидный недостаток: CG@K не нормализована и не учитывает порядок релевантных элементов==
### Discounted Cumulative Gain at K

Discounted Cumulative Gain at K (DCG@K) учитывает порядок элементов в списке путем домножения релевантности элемента на вес равный обратному логарифму номера позиции
$$
GCG@K = \sum_{k=1}^K \dfrac{ 2^{r^{true}(\pi^{-1}(k))} - 1}{ \log_2(k + 1) }.
$$
Если $r^{true}$ принимает только значения 0 и 1, то формула принимает более простой вид
$$
GCG@K = \sum_{k=1}^K \dfrac{ r^{true}(\pi^{-1}(k))}{ \log_2(k + 1) }.
$$
Использование логарифма как функции дисконтирования можно объяснить так. С точки зрения ранжирования позиции в начале списка отличаются гораздо сильнее, чем позиции в его конце. Так в случае поискового движка между позициями 1 и 11 целая пропасть (лишь в нескольких случаях пользователь заходит дальше первой страницы поисковой выдачи), а между позициями 101 и 111 особой разницы нет -- до них мало кто доходит.

То есть
$$
\dfrac{1}{\log_2(1 + 1)} - \dfrac{1}{\log_2(1 + 11)} \approx 0.721 \, \text{и} \, \dfrac{1}{\log_2(1 + 101)} - \dfrac{1}{\log_2(1 + 111)} \approx 0.003
$$

Если CG@K варируется в диапазоне $[0, K]$, то DCG@K уже принимает значения на непонятном отрезке. Эту проблему решает nDCG@K.
### Normalized Discounted Cumulative Gain at K

nDCG@K это нормализованная версия DCG@K
$$
nDCG@K = \dfrac{DCG@K}{IDCG@K},
$$
где $IDCG@K$ -- это максимальное (I -- ideal) значение DCG@K. Так как здесь $r^{true}$ принимает значения $[0, 1]$, то $IDCG@K=\sum_{k=1}^K\dfrac{1}{\log_2(k + 1)}$.

Таким образом, nDCG@K учитывает порядок элементов в списке и принимает значения от 0 до 1.

По аналогии с MAP@K можно усреднить nDCG@K по всем объектам.
### MNAP
$$
MNAP@20 = \dfrac{1}{|U|} \sum_{u \in U} \dfrac{1}{\min (n_u, 20)} \sum_{i=1}^{20} r_u(i)\, p_u@i, \quad p_u@k = \dfrac{1}{k}\sum_{i=1}^k r_u(i),
$$
где $r_u(i)$ -- потребил ли пользователь $u$ контент, предсказанный ему на месте $i$ (1 или 0), $n_u$ --  количество элементов, которые пользователь потребил за тестовый период, $U$ -- множество тестовых пользователей.

Метрика отличается от оригинальной MAP тем, что значение для каждого пользователя нормализуется не на константу, а на число потребленных фильмов. Таким образом, вес угадывания одного фильма больше у пользователей, имеющих меньшее число просмотров. 

### Mean Reciprocal Rank

Mean Reciprocal Rank (MRR) -- еще одна часто используемая метрика ранжирования
$$
MRR@K = \dfrac{1}{N} \sum_{j=1}^N RR@K_j,
$$
где $RR_j$ -- reciprocal rank для $j$-ого объекта; обратный ранг первого правильно угаданного элемента
$$
RR@K = \dfrac{1}{\min \{ k \in [1,\ldots, K]: r^{true}(\pi^{-1}(k)) = 1 \}}.
$$
Mean reciprocal rank изменяется в диапазоне от 0 до 1 и учитывает позицию элементов, ==но только для первого верно угаданного, не обращая внимания на все остальные==.
### Ранговый коэффициент корреляции Кендала

$$
\tau = \dfrac{ (\text{согласованные\,пары}) - (\text{несогласованные\,пары}) }{M \, (M - 1) / 2}
$$
### Ранговый коэффициент корреляции Спирмена

Коэффициент корреляции Спирмена это коэффициент корреляции Пирсона, посчитанный на рангах
$$
r_S = 1 - \dfrac{ \sum_{e \in E} \big( \pi(e) - \pi^{true}(e) \big)^2 }{M \, (M - 1) / 2}.
$$
==Метрики на основе ранговой корреляции не учитывают позицию элемента (еще хуже чем p@K, так как корреляция считается по всем элементам, а не по K элементам с наибольшим рангом) ==. На практике применяется крайне редко.

Есть еще PFound и ERR