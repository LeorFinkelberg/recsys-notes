В задаче ранжирования сначала нужно сформировать набор кандидатов, а уже потом строить финальную сортировку.

Примеры: поиск информации в Интернете. Пользователь задает запрос, и под этот запрос формируется выдача, в которой сайты (документы) расположены по убыванию полезности. $D$ - база веб-страниц, проиндексированных поисковой системой, а $Q$ - множество запросов пользователей. 

==Чтобы решать задачу ранжирования в случае поисковых систем с помощью машинного обучения, необходимо иметь набор данных с оценками ассесоров==. Обычно по парам "запрос - документ" собирают данные о том, насколько документ релевантен запросу. Такие оценки называют _метками релевантности_.

В случае рекомендательных систем объектом может быть пост или видео в социальной сети, а запрос это сам пользователь.
### Метрики качества ранжирования

Предположим, мы решили задачу ранжирования. Обычно это делается обучением некоторой функции от запроса и документа $a_\theta(q, d)$, где $\theta$ - это параметры модели. Если такая функция готова, то выдача по запросу $q \in Q$ получается сортировкой множества $D_q$ по убыванию $a_\theta(q, d)$. 

Пусть $Q_t$ - это множество тестовых запросов, на котором будем оценивать качество решения. $T_K(q)$ - первые $K$ элементов выдачи по запросу $q$, а $d_q^{(k)}$ - $k$-ый по порядку документ в выдаче.

Соответственно, $T_K(q) = d_q^{(1)}, d_q^{(2)}, \ldots, d_q^{(K)}$.
#### Бинарная реливантность

Обычно решение о том релевантен документ запросу или нет, принимают ассесоры -- специальные люди, которые размечают данные для обучения ML-моделей. Собранные оценки называют _метками релевантности_. Условимся обозначать их как $y(q, d)$. 

_Precision / Recall_

Если у релевантности есть всего 2 класса, то можно вспомнить метрики классификации и обобщить их на задачи ранжирования. Полагаем, что ранжирующая модель релевантными документами считает те, которые попали в первые $K$ элементов выдачи, то есть $T_K(q)$. Тогда можно вычислить точность и полноту
$$
Precision@K = \dfrac{\text{Число релевантных документов в топе}}{K} = \dfrac{ \sum_{d \in T_K(q)}I_{y(q, d)} == 1 }{K}
$$
и
$$
Recall@K = \dfrac{\text{число релевантных документов в топе}}{\text{общее число релевантных документов}} = \dfrac{ \sum_{d \in T_K(q)}I_{y(q, d)} == 1 }{\min (K, \sum_{d \in D} I_{y(q, d)} == 1) }.
$$
В случае Recall приходится брать в знаменателе минимум, так как модель не может выявить более $K$ релевантных документов.

_Mean Average Precision_ 

==Метрики Precision и Recall не учитывают порядок документов в выдаче.== Чтобы учесть порядок документов в выдаче, посмотрим на Precision по тем позициям, где стоят релевантные документы и усредним их -- это Average Precision
$$
AP(q) = \dfrac{1}{K} \sum_{k=1}^K Precision@k \cdot y(q, d_q^{(k)}).
$$
NB! Как обсуждалось в чате @ods_recommender_systems, эффект позиционного смещения (когда пользователи более охотно взаимодействуют с айтемами верхних позиций), можно оценить через метрики ранжирования. Можно смотреть, например, на nDCG@k и оптимизировать YetiRankPairwise (Catboost).
